~~ scalability ~~

moving on to 2. point,

# Scalability -
ability of a system to handle increasing load.

load here is a subjective term, can defer from application to application.
 - number of concurrent users.
 - number of requests by a user.
 - size of data processed.

In a System Design Interview, we need to come up with the correct metric to
measure load.

Scalability is often defined using Performance.

-> Performance -
Its a way to mention how quickly our system responds.
example, avg delay or 99th percentile delay.

scalability is measured on acceptable performance per x amount of users.
so, first we define 'an acceptable performance per user' then we check it for x amount
of concurrent users.
And we set the criteria, like, if 90% of the users get acceptable performance then we
will say that the system is scalable for x amount of users.

So, the progression is -
 - define performance
 - define acceptable performance per user.
 - define passing percentage to fulfill a particular criteria for x users.
 - and if the system fulfills that criteria then we declare it to be scalable for x amount of users.


Performance is defined using below two terms -

1. latency -
time taken for a request to get response.

2. throughput -
number of requests that can be delivered within a time period.

An ideal scaling of a system hopes to get the best possible system performance for all concurrent
users simultaneously. And highest Performance is achieved in an ideal scenario when all resources are
available completely for each request.
In that ideal scenario, we will get minimum latency, which every scaling system wishes to have
but with the highest possible throughput. So ideal Scaling wishes for
throughput ∝ 1/latency

But if we try to increase the throughput by keeping the system configuration same, then generally
latency increases as well, because resources gets distributed to manage higher number of requests,
hence, we can say that, in real life, latency ∝ throughput

which as we can see is a conflict between ideal scenario and reality, so there is always a tradeoff,
In reality, good Scaling means increasing Throughput but good Performance means decreasing latency.
So to scale, performance per request is compromised to an acceptable poor level, hence,
while scaling we always strives to achieve best throughput with acceptably poor latency per user.

So, very less latency is not always good, it generally means, throughput of system is
also very low, that means, system is well performing but scale is very low.

latency is a small scale term which just talks about a particular request with respect to a single
user whereas, system working on large scale focuses on throughput.

So, we should maximize throughput, while maintaining acceptable latency.

Acceptable performance is generally defined based on expected throughput of a system.

so,
 - we start with best latency
 - we compromise it to get a good throughput
 - we define acceptable performance (generally called only performance) per user with that compromised latency
 - we decide scale of our system by defining passing percentage for performance for x users.
 - then we declare scale of that system.


latency -> throughput -> performance -> scalability


different methods of scaling -
1. horizontal scaling - adding more machines and dividing the load between them.
2. vertical scaling - improving/increasing resources of a system. It is expensive and not that
efficient, there is finite limit to how much you can scale.