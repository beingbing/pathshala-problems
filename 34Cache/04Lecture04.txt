~~ cache pattern ~~

Now, lets start with cache patterns and how cache is organised/managed/maintained in cache
layers while designing a three-tier system architecture.

1. Cache-Aside Pattern (lazy loading pattern) -
the application code is responsible for managing the cache. When data is requested, the
application first checks the cache. If the data is found, it is returned directly from
the cache. If not, the application retrieves the data from the database, stores it in the
cache for future use, and then returns it to the requester. A well-known example of this
pattern is memCache.

advantages -
- Complete decoupling of the database (DB) and cache, System continues to function if the
cache goes down. A single point of failure in the cache won't affect system operation.
- For read queries: 1 network call for cache hits, 3 for misses. No interaction of write
query with cache layer. Only requested data is stored in the cache, not everything written
to the DB.
- Efficiency with subsequent similar queries within the same time window.

disadvantages -
- Cache misses lead to noticeable delays due to the additional calls.
- Cache invalidation is challenging to detect and implement.

2. Read-Through/Write-Through Pattern -
This pattern combines caching and direct data access to improve data access efficiency in
systems with databases. It's often preferred for read-intensive applications. The cache
layer sits between the database layer and the server layer, handling every query.

- Write-Through: When data is received by the server, it's first written to the cache, which
then updates the underlying database. This ensures synchronization between cache and database.

- Read-Through: If a read query doesn't find the data in the cache, the cache fetches it from
database, adds it to improve cache hits for subsequent reads, and then responds to the server's
request. The cache handles database interactions intelligently.

advantages -
- Reduces the server's workload as the cache manages a lot of the interaction with the database.
- Suitable for scenarios where data consistency is critical and read performance is important i.e.,
for applications frequently reading recently written data.
- Even with cache misses, it can still be more efficient. As,
For read queries: 1 network call for cache hits, 2 for misses.
For write queries: 2 compulsory calls to update data in both cache and DB storage.

disadvantages -
- Tight coupling between cache and database can lead to system failure if the cache fails. To
compensate, if distributed cache layer is created then Handling it can be complex.
- Increased write latency due to the necessity of updating both the cache and the database.
- Can lead to cache pollution if the cache is populated with data that is rarely or never accessed again.

3. Write-Around/Read-Through Pattern -
data is written directly to the underlying data store, bypassing the cache for write operations.
However, for read operations, the cache is utilized. This means that data is only cached when
it is read, not when it is written, reducing the cache pollution with data that is unlikely to
be accessed again soon.

advantage -
- Suitable for scenarios with a mix of read-heavy and write-heavy workloads.
- If cache fails then write will still work.
- Low latency for read queries, with only 1 network call for cache hits and 2 for cache misses.
- Lowest possible latency for write queries, with only 1 network call.

disadvantage -
- Cache invalidation can be challenging.
- cache becomes a single point of failure for read queries.

4. Write-Behind (Write-Back) Pattern -
This pattern involves writing data only to the cache and maintaining queue of all the write
operations initially and then asynchronously bulk update the database periodically at once
in the background. This can improve write performance by clubbing multiple write operations
and executing all at once.

advantages -
- Helpful for managing large volumes of write queries efficiently.
- Provides an extra layer of reliability if the database experiences downtime.

disadvantages -
- If the cache fails, pending writes can be lost, leading to data inconsistency. Some implementations
use message queues instead of cache RAM to mitigate this risk, but it adds complexity to the
system and requires careful maintenance.

5. Refresh Ahead Pattern -
proactively update or refresh data in the cache before it expires, anticipating future requests.
This means that instead of waiting for a request to come in and trigger a cache refresh
(as in typical cache expiration strategies), the cache autonomously updates its contents by
querying DB ahead of time based on usage patterns or predictions. This method is advantageous
in scenarios where we can predict the importance of certain data and the appropriate times to
refresh it automatically.

advantages -
- Faster Response: Data preloading ensures quicker access to information, improving system
responsiveness.
- Lower Latency: Users experience shorter wait times as the required data is already cached,
reducing backend retrieval delays.
- Balanced Workload: By proactively refreshing data, the cache can evenly distribute processing
demands, preventing sudden spikes in activity.

disadvantages -
- Increased Overhead: Implementing proactive data refreshing adds computational and resource
burdens, potentially complicating system operations.
- Resource Consumption: Preloaded data may consume more memory and storage, particularly if
predictions are inaccurate, leading to resource wastage.
- Complexity: Managing predictive algorithms and adapting to dynamic usage patterns can be
challenging, introducing complexity to system maintenance and updates.
