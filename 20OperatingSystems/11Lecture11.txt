~~ shortest time to completion algorithm ~~

so this scheduling in which a running process is interrupted and moved out to give CPU time
to a new process is called pre-emptive scheduling.
so,
every scheduling algorithm can -
    - either be pre-emptive algorithm (it introduces us to context switch)
    - or a non pre-emptive algorithm (no termination is allowed)

now if we remove our fourth assumption as well, then we have to account for I/O time also.

in case of non pre-emptive algorithms, whenever a process comes back from waiting-state it PID
gets changed and it is considered to be a new process hence the CPU time taken by a process
is called CPU burst and I/O time is called as I/O burst.
whereas,
in pre-emptive algorithms, the PID remains the same and it again switched back to ready-state
to be picked for further process.

in case of interactive processes, the I/O burst is very significant with respect to CPU burst,
there is a lot of switching involved to and it may also happen that I/O burst is way more than
CPU burst in total, hence, if we take SJF then the total turnaround time for this process will
be huge and SJF will delegate it for later time but that's not accurate as the CPU burst is
very less and if we consider them separate process then they might be the shortest processes
available in ready-state hence they are given very high priority inspite of it being a big process
because if not then it might start lagging as it needs to be hugely interactive as well and the
user will experience the the lag in realtime.

if we remove our second assumption as well along with first assumption then we can come up for a
better replacement of SJF algorithm, which can be shortest-time-to-completion (STC) algorithm.

pre-emptive SJF (also known as shortest completion time first) is best candidate for processes with
significant I/O bursts.
Every time a new process comes in, we stops the currently running process, decides which is the shortest
and then execute it.

the issue with above process is of starvation. lets say there is a huge process and after every few
second or so a lot of small processes keeps coming in, then the big process gets starved for CPU
time as other process will be given chances before it.

how to deal with starvation ?
