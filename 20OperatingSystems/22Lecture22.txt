~~ translation lookaside buffer ~~

continuing from last lecture.

Although we are still making two step access but cache access <<< RAM access hence we can let the time
taken by cache access be insignificant.

so, with TLB in scene,
every variable access = TLB access + RAM access

if it exist in TLB, it is called TLB hit and then we directly go to RAM address
but in case of TLB miss, we need to go and access page table, then we store it in TLB and then we go
to RAM for access.

what if we go to store in TLB and it is full ?
then we need to have cache replacement policy, which will determine how and which record will be removed
to make space for current entry. The best candidate for this policy is LRU cache replacement policy.

sometimes hackers try to slow system down by failing LRU policy. They do this by making requests of size
n+1 , where n is the count of records that can be stored in cache. hence once the cache is full for every
call, there will be a cache miss.

thus instead of LRU, we go for random replacement policy. It ensures that no one tries to trick the
algorithm into.

benifits of using TLB -
- spatial locality benifit.
if we need to access memory from an offset then if first memory address is in TLB, then all other
access requests becomes very quick. e.g. - if a[0] is in TLB then all other elements access becomes
very efficient and quick.

Also, memory address progression is in row-order, hence next higher address will be adjacent and located
in same row, or goes to next column only if row is full. Thus row-col 2D array access is faster than
col-row 2D array access. As there is high chance for next memory address being in the same page in case
of row-order access which in order have a higher chance of getting a TLB hit.

- temporal locality benifit.
assoicated with time. recently accessed variable has a high chance of being accessed again a few
instructions latter, hence TLB hit probability is high as it will be already present in cache this time.

issues with TLB -
- context switching.
As TLB is not according to a process.
if two process, both have page#3 in execution, so first p1 is getting executed and makes a cache entry
for page#3 :: memory address 105 and after that p2 comes into execution and goes for page#3 as well,
but the entry in TLB was for p1, and will give wrong address to p2.
one solution is to clean TLB at every context switch, but what does cleaning mean ?, to make all the
bits 0, doing that also can create an actual address of memory.
So instead of that,
- we maintain a validity flag, which signifies as the TLB entry is valid for current access or not.
- we will keep address space identifiers (ASIDs) which is equivalent to process IDs (PID) with each
entry in TLB.

----- TLB discussion ends here ------

extra things that we store in page table -
- permissions: if there are multiple pages of 'code' part and for 'stack' part. so normally, code part
is read-only whereas stack-part is read-write. so with each page a permission is maintain for whether
write/read/update is allowed or not.
- shared pages: common library code which can be accessed by multiple processes, but they are read-only.
