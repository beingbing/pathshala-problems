~~ backend scalability: message queues ~~

Beginning our discussion on backend server scalability, both load balancing (LB) and
microservice architecture play vital roles here as well.

LB and microservice architecture aid in scalability by distributing incoming traffic
and breaking down applications into smaller, manageable services.

In addition to them, asynchronous communication is another key factor in scalability.
Unlike synchronous communication, where a client waits for a response, or other client's
request wait for their turn, asynchronous communication allows requests to be processed
independently and no client is blocked for a response. This prevents other requests from
being held up while one is being processed, thus enhancing overall scalability.

Asynchronous communication is broadly of two types -
1. Push Communication: data or events are initiated and transmitted from the sender (server)
to the receiver (client) without the receiver explicitly requesting for them. It is commonly
used for real-time updates, notifications, and event-driven architectures.
- Server-sent events (SSE) example, WebHooks [Half-duplex]
- webSockets [full-duplex] (also sometimes called an extension of long pooling due to persisted connection)

2. Pull Communication: the receiver (client) initiates the request for data or events from
the sender (server) at its own discretion. The server responds to these requests asynchronously
providing the requested information when available. Pull communication is prevalent in
scenarios where clients periodically fetch updates or retrieve data on-demand.
- polling (sometimes called short polling)
- long polling
- streaming

Now, let's explore how asynchronous communication is achieved at the backend using polling -
on receiving a request, backend will immediately respond with a token (response.ok()) indicating
request is in consideration and is presently in queue for getting processed. Once client
receives a token, it shows Loading page on screen and periodically keeps on enquiring for expected
response. This process is known as polling.

In this setup, the backend server acts as the producer of requests, while threads within a
thread-pool act as consumers, picking up jobs from the queue and returning responses after
processing.

In a setup with multiple backend servers, a load balancer (LB) directs incoming requests, and each
server manages its own queue and consumers. However, if a server fails, the queue associated with it
also fails, resulting in lost requests.

To prevent this, we avoid storing queues in volatile memory of servers. Instead, we maintain
a single queue for all backend server replicas. This is called a message-queue.

However, even with persistent memory, if a server goes down, requests remain stuck until server comes
back up. Therefore, we maintain separate queues (request buckets) for each server, and if a server
fails (detected via a heartbeat by LB), requests put in its buckets are redirected to other replicas
using consistent hashing.
